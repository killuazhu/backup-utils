#!/usr/bin/env bash
#/ Usage: ghe-restore-storage <host>
#/
#/ Restore storage objects from an rsync snapshot.
#/
#/ Note: This script typically isn't called directly. It's invoked by the
#/ ghe-restore command.
set -e

# Bring in the backup configuration
# shellcheck source=share/github-backup-utils/ghe-backup-config
. "$( dirname "${BASH_SOURCE[0]}" )/ghe-backup-config"

# Show usage and bail with no arguments
[ -z "$*" ] && print_usage

bm_start "$(basename $0)"

# Grab host arg
GHE_HOSTNAME="$1"

# The snapshot to restore should be set by the ghe-restore command but this lets
# us run this script directly.
: ${GHE_RESTORE_SNAPSHOT:=current}

# Find the objects to restore. We do not need the file size since we data are already laid out.
# storage/d/da/5c/da5c7275ae360ef22756d5621ebb7853dea5869187274fcf2109ec70e9b9fd85
# storage/d/da/5c/da5ce86bcf54e3fadc75105e9bd734fccaaf64144dbd470dd40519a1c6efdf65
storage_paths=$(cd $GHE_DATA_DIR/$GHE_RESTORE_SNAPSHOT/ && find -H storage -mindepth 4 -maxdepth 4 -type f)

# No need to restore anything, early exit
if [ -z "$storage_paths" ]; then
  echo "Warning: Storage backup missing. Skipping ..."
  exit 0
fi

# Perform a host-check and establish GHE_REMOTE_XXX variables.
ghe_remote_version_required "$GHE_HOSTNAME"

# Split host:port into parts
port=$(ssh_port_part "$GHE_HOSTNAME")
host=$(ssh_host_part "$GHE_HOSTNAME")

# Add user / -l option
user="${host%@*}"
[ "$user" = "$host" ] && user="admin"

hostnames=$host
tempdir=$(mktemp -d -t backup-utils-restore-XXXXXX)
remote_tempdir=$(ghe-ssh "$GHE_HOSTNAME" -- mktemp -d -t backup-utils-restore-XXXXXX)
ssh_config_file_opt=
opts="$GHE_EXTRA_SSH_OPTS"
tmp_list=$tempdir/tmp_list
remote_tmp_list=$remote_tempdir/remote_tmp_list
routes_list=$tempdir/routes_list
remote_routes_list=$remote_tempdir/remote_routes_list

if $CLUSTER; then
  ssh_config_file="$tempdir/ssh_config"
  ssh_config_file_opt="-F $tempdir/ssh_config"
  opts="$opts -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o PasswordAuthentication=no"
  hostnames=$(ghe-cluster-nodes "$GHE_HOSTNAME" "storage-server")
  ghe-ssh-config "$GHE_HOSTNAME" "$hostnames" > "$ssh_config_file"
fi

cleanup() {
  rm -rf $tempdir
  ghe-ssh "$GHE_HOSTNAME" -- rm -rf $remote_tempdir
  true
}

trap 'cleanup' EXIT

# The server returns the list of servers where the objects will be sent:
#
# # OID SERVER1 SERVER2 SERVER2
# b8a48b6b122b4ef8175348d1d6fbd846d3b3ccc8fd7552b79f91125c4958e43b server1 server2 server3
# bc4cdd292e6b5387df2a42a907fcd5f3b6804a5d5ab427184faea5ef118d635e server1 server2 server3
# ...
#
# One route per line.
#
# NOTE: The route generation is performed on the appliance as it is considerably
# more performant than performing over an SSH pipe.
#

bm_start "$(basename $0) - Building object list"
# b8a48b6b122b4ef8175348d1d6fbd846d3b3ccc8fd7552b79f91125c4958e43b server1 server2 server3
echo "$storage_paths" | awk -v nodes="$GHE_REMOTE_STORAGE_DATA_NODES" -F/ '{print $NF " " nodes}' > $routes_list
bm_end "$(basename $0) - Building object list"

bm_start "$(basename $0) - Transferring object list"
cat $routes_list | ghe-ssh "$GHE_HOSTNAME" -- sponge $routes_list
ghe_debug "\n$(cat $routes_list)"
bm_end "$(basename $0) - Transferring object list"


if $CLUSTER; then
  bm_start "$(basename $0) - Finalizing routes"
  ghe_verbose "Finalizing routes"
  ghe-ssh "$GHE_HOSTNAME" -- /bin/bash >&3 <<EOF
    split -l 1000 $remote_routes_list $remote_tempdir/chunk
    chunks=\$(find $remote_tempdir/ -name chunk\*)
    parallel -i /bin/sh -c "cat {} | github-env ./bin/storage-cluster-restore-finalize" -- \$chunks
EOF
  bm_end "$(basename $0) - Finalizing routes"
fi

bm_end "$(basename $0)"
